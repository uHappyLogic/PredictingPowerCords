{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "power_cords.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5fVqXJsuhja",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "821eaa0e-8e97-4ba5-b691-9f0ebcc709d6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEcvcaXtuCNr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "519990be-8e4e-4193-952d-012c60973f7c"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk as nltk\n",
        "from sklearn.utils import shuffle, resample\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import datetime\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import time\n",
        "import csv\n",
        "from joblib import dump, load\n",
        "import pickle\n",
        "\n",
        "# ==============================================================================================================\n",
        "# ===================================Config       ==============================================================\n",
        "# ==============================================================================================================\n",
        "\n",
        "NLTK_INITIAL_DOWNLOAD = True\n",
        "PRODUCE_RESULT = False\n",
        "RECALCULATE_DATA = True\n",
        "SAVE_DATA = True\n",
        "TEST_SIZE = 0.05\n",
        "FRACTION = 0.1\n",
        "\n",
        "NLTK_PATH= \"/content/drive/My Drive/data/nltk\"\n",
        "\n",
        "ROOT_PATH= \"/content/drive/My Drive/data/power_cords\"\n",
        "\n",
        "TRAINING_FILE_PATH= \"{}/training.csv\".format(ROOT_PATH)\n",
        "TESTING_FILE_PATH=\"{}/testing.csv\".format(ROOT_PATH)\n",
        "\n",
        "PRECALCULATED_TRAINING_PATH=\"{}/precalculated_training.hdf\".format(ROOT_PATH)\n",
        "PRECALCULATED_VALIDATION_PATH=\"{}/precalculated_validation.hdf\".format(ROOT_PATH)\n",
        "\n",
        "CHOSEN_CLASSIFIER=\"logistic_regresion\"\n",
        "CUSTOM_CLF_ID=\"full_set\"\n",
        "SAVED_CLF_FILE=\"{}/model_{}_{}\".format(ROOT_PATH, CHOSEN_CLASSIFIER, CUSTOM_CLF_ID)\n",
        "SAVED_VECTORIZERS_FILES=SAVED_CLF_FILE + \"_{}.vec\"\n",
        "\n",
        "RESULT_DATA_FILE=SAVED_CLF_FILE + \"_results.csv\"\n",
        "\n",
        "SAVED_CLF_FILE+=\".dump\"\n",
        "\n",
        "\n",
        "PARAMS_COLL = [\n",
        "    'GL',\n",
        "    'ASIN_STATIC_ITEM_NAME',\n",
        "    'ASIN_STATIC_BULLET_POINT',\n",
        "    'ASIN_STATIC_SUBJECT_KEYWORD',\n",
        "    'ASIN_STATIC_ITEM_CLASSIFICATION',\n",
        "    'ASIN_STATIC_ITEM_TYPE_KEYWORD',\n",
        "    #'ASIN_STATIC_MATERIAL',\n",
        "    #'ASIN_STATIC_STYLE',\n",
        "    'ASIN_STATIC_PRODUCT_DESCRIPTION',\n",
        "    'ASIN_STATIC_BRAND',\n",
        "    'ASIN_STATIC_BATTERIES_INCLUDED',\n",
        "    'ASIN_STATIC_BATTERIES_REQUIRED'\n",
        "]\n",
        "\n",
        "COLLS_TRANSFORMS = {\n",
        "        # TODO add tranformers for ASIN_STATIC_BATTERIES_INCLUDED e.g.\n",
        "    }\n",
        "\n",
        "TARGET_COL = 'target_label'\n",
        "ID_COLL = \"ID\"\n",
        "\n",
        "# ==============================================================================================================\n",
        "# ===================================Misc  =====================================================================\n",
        "# ==============================================================================================================\n",
        "\n",
        "\n",
        "PANDAS_STORE_KEY = \"data\"\n",
        "POST_PROCESSED_COLL_MAP = { }\n",
        "POST_PROCESSED_COLL = []\n",
        "\n",
        "for p in PARAMS_COLL:\n",
        "    p_new_name = p + \"_cleared\"\n",
        "    POST_PROCESSED_COLL_MAP[p] = p_new_name\n",
        "    POST_PROCESSED_COLL.append(p_new_name)\n",
        "\n",
        "if PRODUCE_RESULT:\n",
        "    FRACTION = 1.0\n",
        "    TEST_SIZE = 0\n",
        "\n",
        "if NLTK_INITIAL_DOWNLOAD:\n",
        "    nltk.download(\"wordnet\", NLTK_PATH)\n",
        "    nltk.download('stopwords', NLTK_PATH)\n",
        "nltk.data.path.append(NLTK_PATH)\n",
        "\n",
        "class ScopedTimer():\n",
        "    def __init__(self):\n",
        "        self.timed = []\n",
        "        self.last_time = None\n",
        "\n",
        "    def start(self, name):\n",
        "        if self.last_time is None:\n",
        "            self.last_time = time.time()\n",
        "        else:\n",
        "            self.timed[-1] += (str(time.time() - self.last_time),)\n",
        "            self.last_time = time.time()\n",
        "            print(\"{}: {}s\".format(self.timed[-1][0], self.timed[-1][1]))\n",
        "\n",
        "        self.timed.append((name,))\n",
        "\n",
        "    def finish(self):\n",
        "        if self.last_time is not None:\n",
        "            self.timed[-1] += (str(time.time() - self.last_time),)\n",
        "\n",
        "    def print(self):\n",
        "        print(\"[Name{}] [time]\".format(\"\".ljust((15))))\n",
        "        for t in self.timed:\n",
        "            print(\"{} {}s\".format(t[0].ljust(21), t[1]))\n",
        "\n",
        "# ==============================================================================================================\n",
        "# ===================================Load data    ==============================================================\n",
        "# ==============================================================================================================\n",
        "\n",
        "\n",
        "def get_dataset(filename):\n",
        "    return pd.read_csv(filename, encoding='utf-8')\n",
        "\n",
        "\n",
        "def strip_unused_colls(df, used_colls):\n",
        "    return df[used_colls]\n",
        "\n",
        "\n",
        "# this one is training data\n",
        "def shuffle_resample_cut(df, target):\n",
        "    df_0 = df[df[target] == 0]\n",
        "    df_1 = df[df[target] == 1]\n",
        "\n",
        "    df_1 = resample(df_1, replace=True, n_samples=int(len(df_0)))\n",
        "\n",
        "    df = pd.concat([df_0, df_1], axis=0)\n",
        "    df = shuffle(df)\n",
        "\n",
        "    return df.head(int(len(df) * float(FRACTION)))\n",
        "\n",
        "\n",
        "def standarise_and_transform(df, params_colls, colls_transforms):\n",
        "    def standarise(x):\n",
        "        try:\n",
        "            return \"NaN\" if str(x) == \"nan\" else str(x)\n",
        "        except Exception as e:\n",
        "            return x.encode(\"ascii\", \"ignore\")\n",
        "\n",
        "    for p in params_colls:\n",
        "        if p in colls_transforms:\n",
        "            df[p] = df[p].apply(colls_transforms[p])\n",
        "        else:\n",
        "            df[p] = df[p].apply(standarise)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def load_data(filename, used_colls):\n",
        "    df_raw = get_dataset(filename)\n",
        "    df_raw = strip_unused_colls(df_raw, used_colls)\n",
        "    df_raw = standarise_and_transform(df_raw, PARAMS_COLL, COLLS_TRANSFORMS)\n",
        "\n",
        "    return df_raw\n",
        "\n",
        "\n",
        "def load_train_and_validation_data():\n",
        "    scoped_timer.start(\"Load raw data\")\n",
        "\n",
        "    df_raw = load_data(TRAINING_FILE_PATH, [TARGET_COL] + PARAMS_COLL)\n",
        "    df_raw = shuffle_resample_cut(df_raw, TARGET_COL)\n",
        "\n",
        "    df_train, df_validation = train_test_split(df_raw, test_size=TEST_SIZE)\n",
        "\n",
        "    print(\"Validation size {}\".format(len(df_validation.index)))\n",
        "    print(\"Train size {}\".format(len(df_train.index)))\n",
        "\n",
        "    return df_train, df_validation\n",
        "\n",
        "\n",
        "def print_first_n_rows(filenae,df):\n",
        "    print(\"[{}]\".format(filenae))\n",
        "    print(df.head(3))\n",
        "\n",
        "\n",
        "def load_precalculated_data_from_file():\n",
        "    scoped_timer.start(\"Load recalculated data\")\n",
        "    train_df = pd.read_hdf(PRECALCULATED_TRAINING_PATH, PANDAS_STORE_KEY)\n",
        "    validation_df = pd.read_hdf(PRECALCULATED_VALIDATION_PATH, PANDAS_STORE_KEY)\n",
        "\n",
        "    print(\"First 3 rows from recalculated data\")\n",
        "    print_first_n_rows(PRECALCULATED_TRAINING_PATH, train_df)\n",
        "    print_first_n_rows(PRECALCULATED_VALIDATION_PATH, validation_df)\n",
        "\n",
        "    print(\"Validation size {}\".format(len(validation_df.index)))\n",
        "    print(\"Train size {}\".format(len(train_df.index)))\n",
        "\n",
        "    return train_df, validation_df\n",
        "\n",
        "# ==============================================================================================================\n",
        "# ===================================Refine data    ============================================================\n",
        "# ==============================================================================================================\n",
        "\n",
        "\n",
        "def pre_clean(raw_html):\n",
        "    clean_text = raw_html\n",
        "\n",
        "    if not clean_text:\n",
        "        return \"\"\n",
        "\n",
        "    try:\n",
        "        clean_text = clean_text.lower()\n",
        "\n",
        "        replace_with_empty = ['<.*?>', '&#..;', '[0-9]+', '_', '-']\n",
        "        for s in replace_with_empty:\n",
        "            cleanr = re.compile(s)\n",
        "            clean_text = re.sub(cleanr, '', clean_text)\n",
        "            if not clean_text:\n",
        "                return \"\"\n",
        "\n",
        "        replace_with_space_1 = [\n",
        "            '%', '\\?', '\\$', '\\*', '&', ';', ':', '\\?', '\\$', '\\*', '\\[', '\\]', ',', '\\^', '\\.+', '\\\\+', '/']\n",
        "        for s in replace_with_space_1:\n",
        "            cleanr = re.compile(s)\n",
        "            clean_text = re.sub(cleanr, ' ', clean_text)\n",
        "            if not clean_text:\n",
        "                return \"\"\n",
        "\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "    return clean_text\n",
        "\n",
        "\n",
        "def post_clean(txt):\n",
        "    cleantext = txt\n",
        "\n",
        "    if not cleantext:\n",
        "        return \"\"\n",
        "\n",
        "    try:\n",
        "        replace_with_empty = [\"'\"]\n",
        "        for s in replace_with_empty:\n",
        "            cleanr = re.compile(s)\n",
        "            cleantext = re.sub(cleanr, '', cleantext)\n",
        "            if not cleantext:\n",
        "                return \"\"\n",
        "\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "    return cleantext\n",
        "\n",
        "\n",
        "def preprocess_naive(df, params_colls):\n",
        "    scoped_timer.start(\"Refine data\")\n",
        "    unwanted_words = sorted(\n",
        "        [u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u\"you're\", u\"you've\", u\"you'll\",\n",
        "         u\"you'd\", u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u\"she's\",\n",
        "         u'her', u'hers', u'herself', u'it', u\"it's\", u'its', u'itself', u'they', u'them', u'their', u'theirs',\n",
        "         u'themselves', u'what', u'which', u'who', u'whom', u'this', u'that', u\"that'll\", u'these', u'those', u'am',\n",
        "         u'is', u'are', u'was', u'were', u'be', u'been', u'being', u'have', u'has', u'had', u'having', u'do', u'does',\n",
        "         u'did', u'doing', u'a', u'an', u'the', u'and', u'but', u'if', u'or', u'because', u'as', u'until', u'while',\n",
        "         u'of', u'at', u'by', u'for', u'with', u'about', u'against', u'between', u'into', u'through', u'during',\n",
        "         u'before', u'after', u'above', u'below', u'to', u'from', u'up', u'down', u'in', u'out', u'on', u'off', u'over',\n",
        "         u'under', u'again', u'further', u'then', u'once', u'here', u'there', u'when', u'where', u'why', u'how', u'all',\n",
        "         u'any', u'both', u'each', u'few', u'more', u'most', u'other', u'some', u'such', u'nor', u'only', u'own',\n",
        "         u'same', u'so', u'than', u'too', u'very', u's', u't', u'can', u'will', u'just', \"that's\", \"i'd\", \"it's\",\n",
        "         \"i'll\", \"i'm\", \"i've\", \"\", 'against', 'not', 'don', \"don't\", 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\",\n",
        "         'didn', \"didn't\",\n",
        "         'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\",\n",
        "         'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shouldn', \"shouldn't\", 'wasn',\n",
        "         \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"])\n",
        "\n",
        "    def stripper(v):\n",
        "        try:\n",
        "            return ''.join([*filter(str.isalnum, v)])\n",
        "        except:\n",
        "            # print(v)\n",
        "            return \"\"\n",
        "\n",
        "    def refine(x):\n",
        "        return [post_clean(stemmer.stem(word)) for word in\n",
        "                    [word for word in\n",
        "                        [lemmatizer.lemmatize(word) for word in\n",
        "                            [stripper(word) for word in tokenizer.tokenize(\n",
        "                                pre_clean(x)\n",
        "                            )]\n",
        "                        ]\n",
        "                    if (word not in unwanted_words) and (len(word) > 2)]\n",
        "                ]\n",
        "\n",
        "    tokenizer = WhitespaceTokenizer()\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stemmer = SnowballStemmer('english')\n",
        "\n",
        "    for p in params_colls:\n",
        "        df[POST_PROCESSED_COLL_MAP[p]] = df[p].apply(refine)\n",
        "\n",
        "        print(\"Coll '{}' refined \".format(p).ljust(80, \"=\"))\n",
        "        print(df[p].head(3))\n",
        "        print(df[POST_PROCESSED_COLL_MAP[p]].head(3))\n",
        "        del df[p]\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def convert_to_data(df, params_colls):\n",
        "    converted_data = {}\n",
        "    for p in params_colls:\n",
        "        converted_data[p] = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        for p in params_colls:\n",
        "            converted_data[p].append(' '.join(row[POST_PROCESSED_COLL_MAP[p]]))\n",
        "\n",
        "    return converted_data\n",
        "\n",
        "\n",
        "def save_to_hdf(df, filename):\n",
        "    df.to_hdf(filename, PANDAS_STORE_KEY, mode='w')\n",
        "\n",
        "\n",
        "def get_learning_data():\n",
        "    if RECALCULATE_DATA:\n",
        "        train_df, validation_df = load_train_and_validation_data()\n",
        "        train_df = preprocess_naive(train_df, PARAMS_COLL)\n",
        "        validation_df = preprocess_naive(validation_df, PARAMS_COLL)\n",
        "\n",
        "        if SAVE_DATA:\n",
        "            print(\"First 3 rows from saved data\")\n",
        "            print_first_n_rows(PRECALCULATED_TRAINING_PATH, train_df)\n",
        "            print_first_n_rows(PRECALCULATED_VALIDATION_PATH, validation_df)\n",
        "\n",
        "            save_to_hdf(train_df, PRECALCULATED_TRAINING_PATH)\n",
        "            save_to_hdf(validation_df, PRECALCULATED_VALIDATION_PATH)\n",
        "\n",
        "        train_labels = [int(row[TARGET_COL]) for _, row in train_df.iterrows()]\n",
        "        validation_labels = [int(row[TARGET_COL]) for _, row in validation_df.iterrows()]\n",
        "\n",
        "        return train_labels, \\\n",
        "               validation_labels, \\\n",
        "               convert_to_data(train_df, PARAMS_COLL), \\\n",
        "               convert_to_data(validation_df, PARAMS_COLL)\n",
        "    else:\n",
        "        l_train_df, l_validation_df = load_precalculated_data_from_file()\n",
        "        return l_train_df,\\\n",
        "               l_validation_df,\\\n",
        "               convert_to_data(l_train_df, PARAMS_COLL),\\\n",
        "               convert_to_data(l_validation_df, PARAMS_COLL)\n",
        "\n",
        "\n",
        "def get_testing_data():\n",
        "    test_df = load_data(TESTING_FILE_PATH, [ID_COLL] + PARAMS_COLL)\n",
        "\n",
        "    print(\"First 3 rows from saved data\")\n",
        "    print_first_n_rows(TESTING_FILE_PATH, test_df)\n",
        "    test_df = preprocess_naive(test_df, PARAMS_COLL)\n",
        "    return test_df, convert_to_data(test_df, PARAMS_COLL)\n",
        "\n",
        "\n",
        "# ==============================================================================================================\n",
        "# ===================================Create classifier==========================================================\n",
        "# ==============================================================================================================\n",
        "\n",
        "\n",
        "def create_vectorizers(data, params_colls, custom_vectorizers):\n",
        "    vectorizers = {}\n",
        "    for col in params_colls:\n",
        "        if col in custom_vectorizers:\n",
        "            vectorizers[col] = custom_vectorizers[col]\n",
        "        else:\n",
        "            vectorizers[col] = TfidfVectorizer(max_features=500, use_idf=False, ngram_range=(1, 2))\n",
        "\n",
        "        print(\"{} = {}\".format(col, data[col][:3]))\n",
        "        vectorizers[col].fit(data[col])\n",
        "\n",
        "        with open(SAVED_VECTORIZERS_FILES.format(col), 'wb') as fin:\n",
        "            pickle.dump(vectorizers[col], fin)\n",
        "\n",
        "    return vectorizers\n",
        "\n",
        "\n",
        "def load_vectorizers(params_colls):\n",
        "    vectorizers = {}\n",
        "    for col in params_colls:\n",
        "        with open(SAVED_VECTORIZERS_FILES.format(col), 'rb') as fin:\n",
        "            vectorizers[col] = pickle.load(fin)\n",
        "\n",
        "    return vectorizers\n",
        "\n",
        "\n",
        "def create_feed(input_data, params_colls, vectorizers):\n",
        "    features = []\n",
        "\n",
        "    for col in params_colls:\n",
        "        transformed = vectorizers[col].transform(input_data[col]).toarray()\n",
        "        # save memory\n",
        "        del input_data[col]\n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(transformed)\n",
        "        features.append(scaler.transform(transformed))\n",
        "\n",
        "    res = np.column_stack(features)\n",
        "    return res\n",
        "\n",
        "\n",
        "def get_classifier():\n",
        "    ClASSIFIERS = {\n",
        "        'SVM': (lambda: GridSearchCV(\n",
        "            SVC(), [{\n",
        "                'kernel': ['rbf'], 'gamma': [1e-3, 1e-4], 'C': [1, 10, 100, 1000]\n",
        "            },\n",
        "                {\n",
        "                    'kernel': ['linear'], 'C': [1, 10, 100, 1000]\n",
        "                }],\n",
        "            cv=5,\n",
        "            scoring='%s_macro' % 'precision')\n",
        "                ),\n",
        "\n",
        "        'logistic_regresion': (lambda: GridSearchCV(\n",
        "            LogisticRegression(\n",
        "                class_weight='balanced',\n",
        "                solver='lbfgs'),\n",
        "            {'penalty': ['l2'], 'C': [0.5, 1, 1.5, 2, 2.5]},\n",
        "            cv=10)\n",
        "                               )\n",
        "    }\n",
        "\n",
        "    return ClASSIFIERS[CHOSEN_CLASSIFIER]()\n",
        "\n",
        "\n",
        "# ==============================================================================================================\n",
        "# ===================================Train & Validate===========================================================\n",
        "# ==============================================================================================================\n",
        "\n",
        "\n",
        "def train_and_validate():\n",
        "    print(\"Load data\")\n",
        "    train_labels, validation_labels, training_data, validation_data = get_learning_data()\n",
        "\n",
        "    scoped_timer.start(\"Create vectorizers\")\n",
        "\n",
        "    custom_vectorizers = {\n",
        "        'ASIN_STATIC_PRODUCT_DESCRIPTION': TfidfVectorizer(max_features=5000, use_idf=False, ngram_range=(1, 2))\n",
        "    }\n",
        "\n",
        "    vectorizers = create_vectorizers(training_data, PARAMS_COLL, custom_vectorizers)\n",
        "    training_features = create_feed(training_data, PARAMS_COLL, vectorizers)\n",
        "\n",
        "    # to save memory\n",
        "    training_data = None\n",
        "\n",
        "    print('train labels count: ' + str(len(train_labels)))\n",
        "\n",
        "    clf = get_classifier()\n",
        "\n",
        "    scoped_timer.start(\"Train classifier\")\n",
        "    clf.fit(training_features, train_labels)\n",
        "\n",
        "    scoped_timer.start(\"Dump classifier\")\n",
        "    dump(clf, SAVED_CLF_FILE)\n",
        "\n",
        "    scoped_timer.start(\"Create validation data\")\n",
        "\n",
        "    validation_feed = create_feed(validation_data, PARAMS_COLL, vectorizers)\n",
        "\n",
        "    scoped_timer.start(\"Predict on validation data\")\n",
        "    predicted_validation_labels = clf.predict(validation_feed)\n",
        "\n",
        "    conf_matrix = confusion_matrix(validation_labels, predicted_validation_labels)\n",
        "    tn, fp, fn, tp = conf_matrix.ravel()\n",
        "\n",
        "    precision = float(tp / float(float(tp) + float(fp)))\n",
        "    accuracy = (float(tp) + float(tn)) / float(len(validation_labels))\n",
        "\n",
        "    print(\"[Validation confusion matrix]\")\n",
        "    print(conf_matrix)\n",
        "    print('tp:' + str(tp) + '\\nfp:' + str(fp))\n",
        "    print('Precision ' + str(precision))\n",
        "    print('Accuracy ' + str(accuracy))\n",
        "\n",
        "    print('Accuraccy score ' + str(accuracy_score(validation_labels, predicted_validation_labels)))\n",
        "\n",
        "\n",
        "# ==============================================================================================================\n",
        "# ===================================Produce results============================================================\n",
        "# ==============================================================================================================\n",
        "\n",
        "def predict_results():\n",
        "    scoped_timer.start(\"Load classifier\")\n",
        "    clf = load(SAVED_CLF_FILE)\n",
        "\n",
        "    scoped_timer.start(\"Load test data\")\n",
        "    test_df, test_data = get_testing_data()\n",
        "\n",
        "    scoped_timer.start(\"Load vectorizers\")\n",
        "    vectorizers = load_vectorizers(PARAMS_COLL)\n",
        "\n",
        "    testing_feed = create_feed(test_data, PARAMS_COLL, vectorizers)\n",
        "\n",
        "    scoped_timer.start(\"Predict on testing data\")\n",
        "    predicted_testing_labels = clf.predict(testing_feed)\n",
        "\n",
        "    ids = []\n",
        "    for idx, row in test_df.iterrows():\n",
        "        ids.append(row['ID'])\n",
        "\n",
        "    result_data = []\n",
        "\n",
        "    for r_label, r_id in zip(predicted_testing_labels, ids):\n",
        "        result_data.append([str(r_id), str(r_label)])\n",
        "\n",
        "    result_df = pd.DataFrame(result_data, columns=['ID', 'model-score'])\n",
        "    result_df['ID'] = result_df['ID'].astype('str')\n",
        "    result_df['model-score'] = result_df['model-score'].astype('str')\n",
        "\n",
        "    result_df.to_csv(RESULT_DATA_FILE, index=False)\n",
        "\n",
        "    print(\"Results exported \" + str(datetime.datetime.now()))\n",
        "\n",
        "\n",
        "scoped_timer = ScopedTimer()\n",
        "if PRODUCE_RESULT:\n",
        "    predict_results()\n",
        "else:\n",
        "    train_and_validate()\n",
        "scoped_timer.finish()\n",
        "scoped_timer.print()\n",
        "\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /content/drive/My\n",
            "[nltk_data]     Drive/data/nltk...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /content/drive/My\n",
            "[nltk_data]     Drive/data/nltk...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Load data\n",
            "Validation size 2046\n",
            "Train size 38863\n",
            "Load raw data: 5.4545981884002686s\n",
            "Coll 'GL' refined ==============================================================\n",
            "59499        gl_pet_products\n",
            "22344                  gl_pc\n",
            "180668    gl_lawn_and_garden\n",
            "Name: GL, dtype: object\n",
            "59499        [glpetproduct]\n",
            "22344                [glpc]\n",
            "180668    [gllawnandgarden]\n",
            "Name: GL_cleared, dtype: object\n",
            "Coll 'ASIN_STATIC_ITEM_NAME' refined ===========================================\n",
            "59499         DENNERLE Spare Part LED Power Supply Unit 5.0\n",
            "22344     Replacement battery for 5110, 6110, 6150, 7110...\n",
            "180668       Real Life XL Boxer Dog Garden Ornament (SizeA)\n",
            "Name: ASIN_STATIC_ITEM_NAME, dtype: object\n",
            "59499      [dennerl, spare, part, led, power, suppli, unit]\n",
            "22344                                     [replac, batteri]\n",
            "180668    [real, life, boxer, dog, garden, ornament, sizea]\n",
            "Name: ASIN_STATIC_ITEM_NAME_cleared, dtype: object\n",
            "Coll 'ASIN_STATIC_BULLET_POINT' refined ========================================\n",
            "59499     [Spare part power supply unit,For nano power l...\n",
            "22344     [Capacity: 1150mAh,Type: Li-ion,Volts: 3.7V,Si...\n",
            "180668    [Frost resistant.,Made from durable materials,...\n",
            "Name: ASIN_STATIC_BULLET_POINT, dtype: object\n",
            "59499     [spare, part, power, suppli, unit, nano, power...\n",
            "22344     [capac, mah, type, liion, volt, size, certif, ...\n",
            "180668    [frost, resist, made, durabl, materi, hand, fi...\n",
            "Name: ASIN_STATIC_BULLET_POINT_cleared, dtype: object\n",
            "Coll 'ASIN_STATIC_SUBJECT_KEYWORD' refined =====================================\n",
            "59499                                        Nano, Lighting\n",
            "22344                                                   NaN\n",
            "180668    [Real Life,Reallife,Reel Life,Reellife,Vivdart...\n",
            "Name: ASIN_STATIC_SUBJECT_KEYWORD, dtype: object\n",
            "59499                                         [nano, light]\n",
            "22344                                                 [nan]\n",
            "180668    [real, life, reallif, reel, life, reellif, viv...\n",
            "Name: ASIN_STATIC_SUBJECT_KEYWORD_cleared, dtype: object\n",
            "Coll 'ASIN_STATIC_ITEM_CLASSIFICATION' refined =================================\n",
            "59499     base_product\n",
            "22344     base_product\n",
            "180668    base_product\n",
            "Name: ASIN_STATIC_ITEM_CLASSIFICATION, dtype: object\n",
            "59499     [baseproduct]\n",
            "22344     [baseproduct]\n",
            "180668    [baseproduct]\n",
            "Name: ASIN_STATIC_ITEM_CLASSIFICATION_cleared, dtype: object\n",
            "Coll 'ASIN_STATIC_ITEM_TYPE_KEYWORD' refined ===================================\n",
            "59499                NaN\n",
            "22344                NaN\n",
            "180668    outdoor-living\n",
            "Name: ASIN_STATIC_ITEM_TYPE_KEYWORD, dtype: object\n",
            "59499            [nan]\n",
            "22344            [nan]\n",
            "180668    [outdoorliv]\n",
            "Name: ASIN_STATIC_ITEM_TYPE_KEYWORD_cleared, dtype: object\n",
            "Coll 'ASIN_STATIC_PRODUCT_DESCRIPTION' refined =================================\n",
            "59499                                                   NaN\n",
            "22344     <H1 ALIGN=CENTER><FONT SIZE=2>Standard Replace...\n",
            "180668                                         Height: 47cm\n",
            "Name: ASIN_STATIC_PRODUCT_DESCRIPTION, dtype: object\n",
            "59499                                                 [nan]\n",
            "22344     [standard, replac, batteri, product, offer, id...\n",
            "180668                                             [height]\n",
            "Name: ASIN_STATIC_PRODUCT_DESCRIPTION_cleared, dtype: object\n",
            "Coll 'ASIN_STATIC_BRAND' refined ===============================================\n",
            "59499           Dennerle\n",
            "22344       Cameron Sino\n",
            "180668    Vivid Arts Ltd\n",
            "Name: ASIN_STATIC_BRAND, dtype: object\n",
            "59499             [dennerl]\n",
            "22344       [cameron, sino]\n",
            "180668    [vivid, art, ltd]\n",
            "Name: ASIN_STATIC_BRAND_cleared, dtype: object\n",
            "Coll 'ASIN_STATIC_BATTERIES_INCLUDED' refined ==================================\n",
            "59499     false\n",
            "22344     false\n",
            "180668    false\n",
            "Name: ASIN_STATIC_BATTERIES_INCLUDED, dtype: object\n",
            "59499     [fals]\n",
            "22344     [fals]\n",
            "180668    [fals]\n",
            "Name: ASIN_STATIC_BATTERIES_INCLUDED_cleared, dtype: object\n",
            "Coll 'ASIN_STATIC_BATTERIES_REQUIRED' refined ==================================\n",
            "59499     false\n",
            "22344     false\n",
            "180668    false\n",
            "Name: ASIN_STATIC_BATTERIES_REQUIRED, dtype: object\n",
            "59499     [fals]\n",
            "22344     [fals]\n",
            "180668    [fals]\n",
            "Name: ASIN_STATIC_BATTERIES_REQUIRED_cleared, dtype: object\n",
            "Refine data: 122.39322805404663s\n",
            "Coll 'GL' refined ==============================================================\n",
            "89823        gl_toy\n",
            "143705      gl_home\n",
            "96209     gl_sports\n",
            "Name: GL, dtype: object\n",
            "89823       [gltoy]\n",
            "143705     [glhome]\n",
            "96209     [glsport]\n",
            "Name: GL_cleared, dtype: object\n",
            "Coll 'ASIN_STATIC_ITEM_NAME' refined ===========================================\n",
            "89823     Goki Wooden Childrens Play Kitchen With Access...\n",
            "143705    Osram GR10 28 Watt Compact Fluorescent Light C...\n",
            "96209     super natural Base Tee 140 Women's Functional ...\n",
            "Name: ASIN_STATIC_ITEM_NAME, dtype: object\n",
            "89823     [goki, wooden, children, play, kitchen, access...\n",
            "143705    [osram, watt, compact, fluoresc, light, cfl, s...\n",
            "96209     [super, natur, base, tee, woman, function, tsh...\n",
            "Name: ASIN_STATIC_ITEM_NAME_cleared, dtype: object\n",
            "Coll 'ASIN_STATIC_BULLET_POINT' refined ========================================\n",
            "89823     [This is an attractive, colourful and durable ...\n",
            "143705    [Average life: 10,000 hours,Good colour render...\n",
            "96209     [Merino functional shirt lightweight 140g qual...\n",
            "Name: ASIN_STATIC_BULLET_POINT, dtype: object\n",
            "89823     [attract, colour, durabl, toy, stimul, captiv,...\n",
            "143705    [averag, life, hour, good, colour, render, ind...\n",
            "96209     [merino, function, shirt, lightweight, qualiti...\n",
            "Name: ASIN_STATIC_BULLET_POINT_cleared, dtype: object\n",
            "Coll 'ASIN_STATIC_SUBJECT_KEYWORD' refined =====================================\n",
            "89823     [51682,Children,Goki,Kitchen,accessories,boys,...\n",
            "143705    [Compact Fluorescent Lamp,Dulux,Efficiency,Eff...\n",
            "96209     [Atmungsaktiv,BaseLayer,Bekleidung,Camping,EFN...\n",
            "Name: ASIN_STATIC_SUBJECT_KEYWORD, dtype: object\n",
            "89823     [child, goki, kitchen, accessori, boy, girl, i...\n",
            "143705    [compact, fluoresc, lamp, dulux, effici, effic...\n",
            "96209     [atmungsaktiv, baselay, bekleidung, camp, efnd...\n",
            "Name: ASIN_STATIC_SUBJECT_KEYWORD_cleared, dtype: object\n",
            "Coll 'ASIN_STATIC_ITEM_CLASSIFICATION' refined =================================\n",
            "89823     base_product\n",
            "143705    base_product\n",
            "96209     base_product\n",
            "Name: ASIN_STATIC_ITEM_CLASSIFICATION, dtype: object\n",
            "89823     [baseproduct]\n",
            "143705    [baseproduct]\n",
            "96209     [baseproduct]\n",
            "Name: ASIN_STATIC_ITEM_CLASSIFICATION_cleared, dtype: object\n",
            "Coll 'ASIN_STATIC_ITEM_TYPE_KEYWORD' refined ===================================\n",
            "89823         NaN\n",
            "143705    artwork\n",
            "96209         NaN\n",
            "Name: ASIN_STATIC_ITEM_TYPE_KEYWORD, dtype: object\n",
            "89823         [nan]\n",
            "143705    [artwork]\n",
            "96209         [nan]\n",
            "Name: ASIN_STATIC_ITEM_TYPE_KEYWORD_cleared, dtype: object\n",
            "Coll 'ASIN_STATIC_PRODUCT_DESCRIPTION' refined =================================\n",
            "89823     Cooking fun and enjoyment for your little chef...\n",
            "143705    2D compact fluorescent lamp, GR10q/28W-827, 4-...\n",
            "96209                                                   NaN\n",
            "Name: ASIN_STATIC_PRODUCT_DESCRIPTION, dtype: object\n",
            "89823     [cook, fun, enjoy, littl, chef, age, year, hei...\n",
            "143705      [compact, fluoresc, lamp, grq, pin, cfl, squar]\n",
            "96209                                                 [nan]\n",
            "Name: ASIN_STATIC_PRODUCT_DESCRIPTION_cleared, dtype: object\n",
            "Coll 'ASIN_STATIC_BRAND' refined ===============================================\n",
            "89823             GoKi\n",
            "143705           Osram\n",
            "96209     Supernatural\n",
            "Name: ASIN_STATIC_BRAND, dtype: object\n",
            "89823           [goki]\n",
            "143705         [osram]\n",
            "96209     [supernatur]\n",
            "Name: ASIN_STATIC_BRAND_cleared, dtype: object\n",
            "Coll 'ASIN_STATIC_BATTERIES_INCLUDED' refined ==================================\n",
            "89823     false\n",
            "143705    false\n",
            "96209     false\n",
            "Name: ASIN_STATIC_BATTERIES_INCLUDED, dtype: object\n",
            "89823     [fals]\n",
            "143705    [fals]\n",
            "96209     [fals]\n",
            "Name: ASIN_STATIC_BATTERIES_INCLUDED_cleared, dtype: object\n",
            "Coll 'ASIN_STATIC_BATTERIES_REQUIRED' refined ==================================\n",
            "89823     false\n",
            "143705    false\n",
            "96209       NaN\n",
            "Name: ASIN_STATIC_BATTERIES_REQUIRED, dtype: object\n",
            "89823     [fals]\n",
            "143705    [fals]\n",
            "96209      [nan]\n",
            "Name: ASIN_STATIC_BATTERIES_REQUIRED_cleared, dtype: object\n",
            "First 3 rows from saved data\n",
            "[/content/drive/My Drive/data/power_cords/precalculated_training.hdf]\n",
            "        target_label  ... ASIN_STATIC_BATTERIES_REQUIRED_cleared\n",
            "59499            0.0  ...                                 [fals]\n",
            "22344            0.0  ...                                 [fals]\n",
            "180668           1.0  ...                                 [fals]\n",
            "\n",
            "[3 rows x 11 columns]\n",
            "[/content/drive/My Drive/data/power_cords/precalculated_validation.hdf]\n",
            "        target_label  ... ASIN_STATIC_BATTERIES_REQUIRED_cleared\n",
            "89823            1.0  ...                                 [fals]\n",
            "143705           1.0  ...                                 [fals]\n",
            "96209            1.0  ...                                  [nan]\n",
            "\n",
            "[3 rows x 11 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py:2377: PerformanceWarning: \n",
            "your performance may suffer as PyTables will pickle object types that it cannot\n",
            "map directly to c-types [inferred_type->mixed,key->block1_values] [items->['GL_cleared', 'ASIN_STATIC_ITEM_NAME_cleared', 'ASIN_STATIC_BULLET_POINT_cleared', 'ASIN_STATIC_SUBJECT_KEYWORD_cleared', 'ASIN_STATIC_ITEM_CLASSIFICATION_cleared', 'ASIN_STATIC_ITEM_TYPE_KEYWORD_cleared', 'ASIN_STATIC_PRODUCT_DESCRIPTION_cleared', 'ASIN_STATIC_BRAND_cleared', 'ASIN_STATIC_BATTERIES_INCLUDED_cleared', 'ASIN_STATIC_BATTERIES_REQUIRED_cleared']]\n",
            "\n",
            "  return pytables.to_hdf(path_or_buf, key, self, **kwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Refine data: 25.882829666137695s\n",
            "GL = ['glpetproduct', 'glpc', 'gllawnandgarden']\n",
            "ASIN_STATIC_ITEM_NAME = ['dennerl spare part led power suppli unit', 'replac batteri', 'real life boxer dog garden ornament sizea']\n",
            "ASIN_STATIC_BULLET_POINT = ['spare part power suppli unit nano power led led power suppli unit spare part', 'capac mah type liion volt size certif iso roh', 'frost resist made durabl materi hand finish extremeley realist life like eye high collect']\n",
            "ASIN_STATIC_SUBJECT_KEYWORD = ['nano light', 'nan', 'real life reallif reel life reellif vivdart vivid art bone box boxer cruft dog fur garden guard hound lifelik pedigre pooch resin statu']\n",
            "ASIN_STATIC_ITEM_CLASSIFICATION = ['baseproduct', 'baseproduct', 'baseproduct']\n",
            "ASIN_STATIC_ITEM_TYPE_KEYWORD = ['nan', 'nan', 'outdoorliv']\n",
            "ASIN_STATIC_PRODUCT_DESCRIPTION = ['nan', 'standard replac batteri product offer ideal solut exist batteri develop fault need spare period heavi usag would drain first batteri use finest qualiti batteri cell reli replac batteri offer equival superior perform origin pda batteri', 'height']\n",
            "ASIN_STATIC_BRAND = ['dennerl', 'cameron sino', 'vivid art ltd']\n",
            "ASIN_STATIC_BATTERIES_INCLUDED = ['fals', 'fals', 'fals']\n",
            "ASIN_STATIC_BATTERIES_REQUIRED = ['fals', 'fals', 'fals']\n",
            "train labels count: 38863\n",
            "Create vectorizers: 35.043055057525635s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train classifier: 2521.8138105869293s\n",
            "Dump classifier: 0.012123823165893555s\n",
            "Create validation data: 1.1949987411499023s\n",
            "[Validation confusion matrix]\n",
            "[[777 281]\n",
            " [190 798]]\n",
            "tp:798\n",
            "fp:281\n",
            "Precision 0.7395736793327155\n",
            "Accuracy 0.7697947214076246\n",
            "Accuraccy score 0.7697947214076246\n",
            "[Name               ] [time]\n",
            "Load raw data         5.4545981884002686s\n",
            "Refine data           122.39322805404663s\n",
            "Refine data           25.882829666137695s\n",
            "Create vectorizers    35.043055057525635s\n",
            "Train classifier      2521.8138105869293s\n",
            "Dump classifier       0.012123823165893555s\n",
            "Create validation data 1.1949987411499023s\n",
            "Predict on validation data 0.372112512588501s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
